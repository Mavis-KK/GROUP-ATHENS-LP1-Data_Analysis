{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been tasked to analyse the datasets provided (Indian Start-up Funding 2018-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing appropriate packages\n",
    "import warnings\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignoring Warnings that might arise throughout the project\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the 2018 Dataset and Cleaning It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pandas to load the csv file needed to be cleaned\n",
    "\n",
    "df_18 = pd.read_csv('./data/startup_funding2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a Glimpse of the Dataset\n",
    "\n",
    "df_18.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the number of rows and the number of columns in the 2018 Dataset\n",
    "\n",
    "df_18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting some statistical values from the dataset \n",
    "\n",
    "df_18.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the Column Names of the dataset\n",
    "\n",
    "df_18.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'Location' column was in the format, 'City, Region, Country', but we needed only the 'City' aspect of the value\n",
    "# Thus taking all character until we reach a comma sign\n",
    "\n",
    "df_18[\"Location\"] = df_18['Location'].str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From obersavtion: One notes the use of official and unofficial names of certain cities, which when not rectified\n",
    "# will not give actual figures relating to a city with such perculiarity. A city with more than one name.\n",
    "\n",
    "# This is for looking for columns with City Bengaluru\n",
    "\n",
    "bengaluru = df_18.loc[df_18['Location'] == 'Bengaluru'].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city Bengaluru\n",
    "\n",
    "if(bengaluru['Location'].sum() > 0):\n",
    "    \n",
    "    # Replacing the all occurances of Bengaluru and Bangalore City as Bangalore\n",
    "    \n",
    "    df_18['Location'] = df_18['Location'].str.replace('Bengaluru','Bangalore')\n",
    "    df_18['Location'] = df_18['Location'].str.replace('Bangalore City','Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From obersavtion: One notes the use of official and unofficial names of certain cities, which when not rectified\n",
    "# will not give actual figures relating to a city with such perculiarity. A city with more than one name.\n",
    "\n",
    "# This is for looking for columns with City Gurugram\n",
    "\n",
    "gurugram = df_18.loc[df_18['Location'] == 'Gurugram'].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city Bengaluru\n",
    "\n",
    "if(gurugram['Location'].sum() > 0):\n",
    "    \n",
    "    # Replacing the all occurances of Gurugram as Gurgaon\n",
    "    \n",
    "    df_18['Location'] = df_18['Location'].str.replace('Gurugram','Gurgaon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for looking for columns with City New Delhi\n",
    "\n",
    "delhi = df_18.loc[df_18['Location'] == 'New Delhi'].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city New Delhi\n",
    "\n",
    "if(delhi['Location'].sum() > 0):\n",
    "    # Replacing the all occurances of Gurugram as Gurgaon\n",
    "    \n",
    "    df_18['Location'] = df_18['Location'].str.replace('New Delhi','Delhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns so that it matches with that of the other year's dataset\n",
    "\n",
    "df_18.rename(columns = {'Company Name':'Company/Brand',\n",
    "                        'Industry':'Sector',\n",
    "                        'Amount':'Amount($)',\n",
    "                        'Location':'HeadQuarter',\n",
    "                        'About Company':'What it does'}, \n",
    "             inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the renaming of columns \n",
    "\n",
    "df_18.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column called Funding Year, so that when the four datasets are merged later, \n",
    "# it will be easier to associate each row to its year of funding\n",
    "\n",
    "df_18.insert(5,\"Funding Year\", 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging the columns to correspond to that of the \n",
    "df_18 = df_18.reindex(columns=['Company/Brand',\n",
    "                               'HeadQuarter', \n",
    "                               'Sector', \n",
    "                               'What it does',\n",
    "                               'Amount($)',\n",
    "                               'Funding Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After careful examination, the Amount column is supposed to be in USD but there are some that in Indian Rupees (INR)\n",
    "\n",
    "rupees = df_18[df_18[\"Amount($)\"].str.startswith(\"₹\")]\n",
    "\n",
    "# Displaying all rows with the INR sign\n",
    "rupees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the rupee values need to be converted to USD, there is a site that gives the yearly historic exchange rate of currencies\n",
    "# against the USD. \n",
    "\n",
    "# Downloaded the yearly historical exchange rate for the years 2018-2021 \n",
    "# from https://data.oecd.org/conversion/exchange-rates.htm \n",
    "\n",
    "# Loading the yearly historical exchange rate for the years 2018-2021\n",
    "\n",
    "exchange_rate = pd.read_csv('./data/historic_exchange_rate.csv')\n",
    "\n",
    "# Displaying the values from the .csv\n",
    "\n",
    "exchange_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for the dataset concerning India\n",
    "\n",
    "indian_usd_rate = exchange_rate.loc[exchange_rate['LOCATION'] == 'IND']\n",
    "\n",
    "# Displaying rows for INR/USD yearly exchange rate for the years 2018-2021\n",
    "\n",
    "indian_usd_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting specifically the value for the exchange rate between INR and USD for the year 2018\n",
    "\n",
    "value = exchange_rate.query(\"LOCATION == 'IND' and TIME == 2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting value returned into a float for currency conversion later on\n",
    "\n",
    "rate_value = float(value['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column that would be used as a temporary holding column to help indicate whether a value on the Amount($) column \n",
    "# is in INR, USD or -\n",
    "\n",
    "df_18['Amount(USD)'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the Amount($) starts with ₹, it would be represented by the 'R' on the newly created column\n",
    "df_18.loc[df_18['Amount($)'].str.startswith(\"₹\"), 'Amount(USD)'] = 'R'\n",
    "\n",
    "# If the Amount($) starts with $, it would be represented by the 'U' on the newly created column\n",
    "df_18.loc[df_18['Amount($)'].str.startswith(\"$\"), 'Amount(USD)'] = 'U'\n",
    "\n",
    "# If the Amount($) is —, it would be represented by the 'E' on the newly created column\n",
    "df_18.loc[df_18['Amount($)'] == \"—\", 'Amount(USD)'] = 'E'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Striping off the values in Amount($), the currency signs (₹ and $)\n",
    "\n",
    "df_18.loc[df_18['Amount($)'].str.startswith(\"₹\"), 'Amount($)'] = df_18['Amount($)'].str[1:]\n",
    "df_18.loc[df_18['Amount($)'].str.startswith(\"$\"), 'Amount($)'] = df_18['Amount($)'].str[1:]\n",
    "\n",
    "# Assigning 1.0 to the rows that have \"-\" as value\n",
    "\n",
    "df_18.loc[df_18['Amount($)'].str.startswith(\"—\"), 'Amount($)'] = '1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all commas from the values of Amount($)\n",
    "\n",
    "df_18.loc[df_18['Amount($)'].str.contains(',', regex=True), 'Amount($)'] = df_18['Amount($)'].str.replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the Amount($) is void of any non numerical characters, Converting the column to a float\n",
    "\n",
    "df_18['Amount($)'] = df_18['Amount($)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all amount that had the INR sign on them to its USD equivalence using the value from the yearly historic exchange\n",
    "# rate, making reference to the Amount(USD) table that was created above\n",
    "\n",
    "df_18.loc[df_18['Amount(USD)'] == 'R', 'Amount($)'] = df_18['Amount($)'] / rate_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the temporal column created to make reference to those row that had other currencies other than USD\n",
    "\n",
    "df_18.drop(columns=['Amount(USD)'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A preview of the cleaned dataset for 2018\n",
    "\n",
    "df_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned dataset as prepped_2018.csv. Waiting for other members assigned to the other year's dataset\n",
    "# So that it would be merged\n",
    "\n",
    "df_18.to_csv('./data/prepped_2018.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the 2020 Dataset and Cleaning It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pandas to load the csv file needed to be cleaned\n",
    "\n",
    "df_20 = pd.read_csv('./data/startup_funding2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Taking a Glimpse of the Dataset\n",
    "\n",
    "df_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the Column Names of the dataset\n",
    "\n",
    "df_20.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the columns that are not important to our analysis including an unnamed column\n",
    "\n",
    "df_20.drop(columns=df_20.columns[[1,5,6,8,-1]],  axis=1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column called Funding Year, so that when the four datasets are merged later, \n",
    "# it will be easier to associate each row to its year of funding\n",
    "\n",
    "df_20.insert(5,\"Funding Year\", 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the dataset\n",
    "\n",
    "df_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'Location' column was in the format, 'City, Region, Country', but we needed only the 'City' aspect of the value\n",
    "# Thus taking all character until we reach a comma sign\n",
    "\n",
    "df_20[\"HeadQuarter\"] = df_20['HeadQuarter'].str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From obersavtion: One notes the use of official and unofficial names of certain cities, which when not rectified\n",
    "# will not give actual figures relating to a city with such perculiarity. A city with more than one name.\n",
    "\n",
    "# This is for looking for columns with City Gurugram\n",
    "\n",
    "gurugram = df_20.loc[df_20['HeadQuarter'] == 'Gurugram'].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city Gurugram\n",
    "\n",
    "if(gurugram['HeadQuarter'].sum() > 0):\n",
    "    # Replacing the all occurances of Gurugram as Gurgaon\n",
    "    df_20['HeadQuarter'] = df_20['HeadQuarter'].str.replace('Gurugram','Gurgaon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From obersavtion: One notes the use of official and unofficial names of certain cities, which when not rectified\n",
    "# will not give actual figures relating to a city with such perculiarity. A city with more than one name.\n",
    "\n",
    "# This is for looking for columns with City Bengaluru\n",
    "\n",
    "bengaluru = df_20.loc[(df_20['HeadQuarter'] == 'Bengaluru')].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city Bengaluru\n",
    "\n",
    "if(bengaluru['HeadQuarter'].sum() > 0):\n",
    "    # Replacing the all occurances of Bengaluru and Bangalore City as Bangalore\n",
    "    \n",
    "    df_20['HeadQuarter'] = df_20['HeadQuarter'].str.replace('Bengaluru','Bangalore')\n",
    "    df_20['HeadQuarter'] = df_20['HeadQuarter'].str.replace('Bangalore City','Bangalore')\n",
    "    df_20['HeadQuarter'] = df_20['HeadQuarter'].str.replace('Banglore','Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for looking for columns with City New Delhi\n",
    "\n",
    "delhi = df_20.loc[df_20['HeadQuarter'] == 'New Delhi'].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city New Delhi\n",
    "\n",
    "if(delhi['HeadQuarter'].sum() > 0):\n",
    "    # Replacing the all occurances of New Delhi as Delhi\n",
    "    \n",
    "    df_20['HeadQuarter'] = df_20['HeadQuarter'].str.replace('New Delhi','Delhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing a wrongly spelt Hyderabad\n",
    "\n",
    "df_20.loc[df_20['HeadQuarter'] == 'Hyderebad', 'HeadQuarter'] = 'Hyderabad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20[df_20['Amount($)'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping these rows with null values as it becomes impossible to the run subsequent process\n",
    "\n",
    "df_20 = df_20[df_20['Amount($)'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Striping off the values in Amount($), the $ currency sign\n",
    "df_20.loc[df_20['Amount($)'].str.startswith('$'), 'Amount($)'] = df_20['Amount($)'].str[1:]\n",
    "\n",
    "# Removing all commas from the values of Amount($)\n",
    "df_20.loc[df_20['Amount($)'].str.contains(',', regex=True), 'Amount($)'] = df_20['Amount($)'].str.replace(',','')\n",
    "\n",
    "# There was a funding but it was stated as a range, the average of the range was used\n",
    "df_20.loc[df_20['Amount($)'] == '800000000 to $850000000', 'Amount($)'] = '825000000'\n",
    "\n",
    "# Assigning all undisclosed amount as 1.0, since there was a funding, it was just not disclosed\n",
    "# and also taking care of those Undisclosed values that were wrongly spelt\n",
    "df_20.loc[(df_20['Amount($)'] == 'Undiclsosed') \n",
    "          | (df_20['Amount($)'] == 'Undislosed')\n",
    "          | (df_20['Amount($)'] == 'Undisclosed'), 'Amount($)'] = '1.0'\n",
    "\n",
    "# Assigning the value 28 million as verified from the site:\n",
    "# https://techcrunch.com/2020/11/18/payments-app-true-balance-raises-28-million-to-reach-more-underbanked-users-in-india/\n",
    "df_20.at[465, 'Amount($)'] = '28000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the column Amonut($) to float data type\n",
    "\n",
    "df_20['Amount($)'] = df_20['Amount($)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned dataset as prepped_2018.csv. Waiting for other members assigned to the other year's dataset\n",
    "# So that it would be merged\n",
    "\n",
    "df_20.to_csv('./data/prepped_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the 2019 Dataset and Cleaning It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sing Pandas to load the csv file needed to be cleaned\n",
    "\n",
    "df_19 = pd.read_csv('./data/startup_funding2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a Glimpse of the Dataset\n",
    "\n",
    "df_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the columns that are not important to our analysis including an unnamed column\n",
    "\n",
    "df_19.drop(columns=['Founded','Founders','Investor','Stage'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column called Funding Year, so that when the four datasets are merged later, \n",
    "# it will be easier to associate each row to its year of funding\n",
    "\n",
    "df_19.insert(5,\"Funding Year\", 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the dataset\n",
    "\n",
    "df_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'Location' column was in the format, 'City, Region, Country', but we needed only the 'City' aspect of the value\n",
    "# Thus taking all character until we reach a comma sign\n",
    "\n",
    "df_19[\"HeadQuarter\"] = df_19['HeadQuarter'].str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From obersavtion: One notes the use of official and unofficial names of certain cities, which when not rectified\n",
    "# will not give actual figures relating to a city with such perculiarity. A city with more than one name.\n",
    "\n",
    "# This is for looking for columns with City Gurugram\n",
    "\n",
    "gurugram = df_19.loc[df_19['HeadQuarter'] == 'Gurugram'].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city Gurugram\n",
    "\n",
    "if(gurugram['HeadQuarter'].sum() > 0):\n",
    "    # Replacing the all occurances of Gurugram as Gurgaon\n",
    "    df_19['HeadQuarter'] = df_19['HeadQuarter'].str.replace('Gurugram','Gurgaon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From obersavtion: One notes the use of official and unofficial names of certain cities, which when not rectified\n",
    "# will not give actual figures relating to a city with such perculiarity. A city with more than one name.\n",
    "\n",
    "# This is for looking for columns with City Bengaluru\n",
    "\n",
    "bengaluru = df_19.loc[(df_19['HeadQuarter'] == 'Bengaluru')].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city Bengaluru\n",
    "\n",
    "if(bengaluru['HeadQuarter'].sum() > 0):\n",
    "    # Replacing the all occurances of Bengaluru and Bangalore City as Bangalore\n",
    "    df_19['HeadQuarter'] = df_19['HeadQuarter'].str.replace('Bengaluru','Bangalore')\n",
    "    df_19['HeadQuarter'] = df_19['HeadQuarter'].str.replace('Bangalore City','Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for looking for columns with City New Delhi\n",
    "\n",
    "delhi = df_19.loc[df_19['HeadQuarter'] == 'New Delhi'].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city New Delhi\n",
    "\n",
    "if(delhi['HeadQuarter'].sum() > 0):\n",
    "    # Replacing the all occurances of New Delhi as Delhi\n",
    "    \n",
    "    df_19['HeadQuarter'] = df_18['HeadQuarter'].str.replace('New Delhi','Delhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Striping off the values in Amount($), the $ currency sign\n",
    "df_19.loc[df_19['Amount($)'].str.startswith('$'), 'Amount($)'] = df_19['Amount($)'].str[1:]\n",
    "\n",
    "# Assigning all undisclosed amount as 1.0, since there was a funding, it was just not disclosed\n",
    "df_19.loc[df_19['Amount($)'] == 'Undisclosed', 'Amount($)'] = '1.0'\n",
    "\n",
    "# Removing all commas from the values of Amount($)\n",
    "df_19.loc[df_19['Amount($)'].str.contains(',', regex=True), 'Amount($)'] = df_19['Amount($)'].str.replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying changes made to dataset\n",
    "df_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the column Amonut($) to float data type\n",
    "\n",
    "df_19['Amount($)'] = df_19['Amount($)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned dataset as prepped_2019.csv. Waiting for other members assigned to the other year's dataset\n",
    "# So that it would be merged\n",
    "\n",
    "df_19.to_csv('./data/prepped_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the 2021 Dataset and Cleaning It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pandas to load the csv file needed to be cleaned\n",
    "\n",
    "df_21 = pd.read_csv('./data/startup_funding2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a Glimpse of the Dataset\n",
    "\n",
    "df_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the Column Names of the dataset\n",
    "\n",
    "df_21.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the columns that are not important to our analysis including an unnamed column\n",
    "\n",
    "df_21.drop(columns=df_21.columns[[1,5,6,8]],  axis=1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column called Funding Year, so that when the four datasets are merged later, \n",
    "# it will be easier to associate each row to its year of funding\n",
    "\n",
    "df_21.insert(5,\"Funding Year\", 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the dataset\n",
    "\n",
    "df_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'Location' column was in the format, 'City, Region, Country', but we needed only the 'City' aspect of the value\n",
    "# Thus taking all character until we reach a comma sign\n",
    "\n",
    "df_21[\"HeadQuarter\"] = df_21['HeadQuarter'].str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From obersavtion: One notes the use of official and unofficial names of certain cities, which when not rectified\n",
    "# will not give actual figures relating to a city with such perculiarity. A city with more than one name.\n",
    "\n",
    "# This is for looking for columns with City Gurugram\n",
    "\n",
    "gurugram = df_21.loc[df_21['HeadQuarter'] == 'Gurugram'].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city Gurugram\n",
    "\n",
    "if(gurugram['HeadQuarter'].sum() > 0):\n",
    "    # Replacing the all occurances of Gurugram as Gurgaon\n",
    "    df_21['HeadQuarter'] = df_21['HeadQuarter'].str.replace('Gurugram','Gurgaon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From obersavtion: One notes the use of official and unofficial names of certain cities, which when not rectified\n",
    "# will not give actual figures relating to a city with such perculiarity. A city with more than one name.\n",
    "\n",
    "# This is for looking for columns with City Bengaluru\n",
    "\n",
    "bengaluru = df_21.loc[(df_21['HeadQuarter'] == 'Bengaluru')].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city Bengaluru\n",
    "\n",
    "if(bengaluru['HeadQuarter'].sum() > 0):\n",
    "    # Replacing the all occurances of Bengaluru and Bangalore City as Bangalore\n",
    "    df_21['HeadQuarter'] = df_21['HeadQuarter'].str.replace('Bengaluru','Bangalore')\n",
    "    df_21['HeadQuarter'] = df_21['HeadQuarter'].str.replace('Bangalore City','Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for looking for columns with City New Delhi\n",
    "\n",
    "delhi = df_21.loc[df_21['HeadQuarter'] == 'New Delhi'].count()\n",
    "\n",
    "# Testing whether there are any such occurence of the city New Delhi\n",
    "\n",
    "if(delhi['HeadQuarter'].sum() > 0):\n",
    "    # Replacing the all occurances of New Delhi as Delhi\n",
    "    df_21['HeadQuarter'] = df_21['HeadQuarter'].str.replace('New Delhi','Delhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_21.loc[df_21['HeadQuarter'] == 'Ahmadabad', 'HeadQuarter'] = 'Ahmedabad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for null values\n",
    "\n",
    "df_21[df_21['Amount($)'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping these rows with null values as it becomes impossible to the run subsequent process\n",
    "\n",
    "df_21 = df_21[df_21['Amount($)'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Striping off the values in Amount($), those starting with $$ currency sign\n",
    "df_21.loc[df_21['Amount($)'].str.startswith('$$'), 'Amount($)'] = df_21['Amount($)'].str[1:]\n",
    "\n",
    "# Striping off the values in Amount($), the $ currency sign\n",
    "df_21.loc[df_21['Amount($)'].str.startswith('$'), 'Amount($)'] = df_21['Amount($)'].str[1:]\n",
    "\n",
    "# Removing all commas from the values of Amount($)\n",
    "df_21.loc[df_21['Amount($)'].str.contains(',', regex=True), 'Amount($)'] = df_21['Amount($)'].str.replace(',','')\n",
    "\n",
    "# Assigning all undisclosed amount as 1.0, since there was a funding, it was just not disclosed\n",
    "# and also taking care of those Undisclosed values that were uniquely spelt\n",
    "df_21.loc[(df_21['Amount($)'] == 'Undisclosed')\n",
    "        | (df_21['Amount($)'] == 'undisclosed'), 'Amount($)'] = '1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing empty space with null the Amount($) column \n",
    "df_21.loc[df_21['Amount($)'].str.contains(' ', regex=True), 'Amount($)'] = df_21['Amount($)'].str.replace(' ','')\n",
    "\n",
    "# Treating null values as undisclosed amount thus assigning them 1.0 per our convention\n",
    "df_21.loc[(df_21['Amount($)'] == ''), 'Amount($)'] = '1.0'\n",
    "\n",
    "# Rectifying the values that had other characters other than figures\n",
    "df_21['Amount($)']=[re.sub('[^\\w\\s]+', '', s) for s in df_21['Amount($)'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_21.drop(df_21[(df_21['Amount($)'].str.isalpha())].index , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_21.loc[df_21['Amount($)'] == '']\n",
    "#df_21.drop([1137,1146] , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Amount($) as a float data type\n",
    "\n",
    "df_21['Amount($)'] = df_21['Amount($)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the changes made to the dataset\n",
    "\n",
    "df_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned dataset as prepped_2019.csv. Waiting for other members assigned to the other year's dataset\n",
    "# So that it would be merged\n",
    "\n",
    "df_21.to_csv('./data/prepped_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Prepped Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring dataframes to read the prepped csv from above\n",
    "\n",
    "# Laoding prepped 2018 dataset\n",
    "df1 = pd.read_csv('./data/prepped_2018.csv')\n",
    "# Laoding prepped 2019 dataset\n",
    "df2 = pd.read_csv('./data/prepped_2019.csv')\n",
    "# Laoding prepped 2020 dataset\n",
    "df3 = pd.read_csv('./data/prepped_2020.csv')\n",
    "# Laoding prepped 2021 dataset\n",
    "df4 = pd.read_csv('./data/prepped_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [df1,df2,df3,df4]\n",
    "\n",
    "# Merging the prepped datasets into a merged dataframe\n",
    "merged_data = pd.concat(dataset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peeking at the merged data\n",
    "\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the merged data in another csv\n",
    "\n",
    "merged_data.to_csv('./data/merged_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.dropna(subset=['HeadQuarter','Sector'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Regrouping the Sector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Sector'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regrouping the Sector column into the Major Sectors of the Economy\n",
    "# Grouping FinTech as part of the Financial Sector as it is the dominant function of Fintech. Fintech just leverages\n",
    "# the technology aspect to facilitate transactions. So is Edutech, using the same logic.\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('gri', regex=True))              |\n",
    "                (merged_data['Sector'].str.contains('Nutrition sector', regex=True)) | \n",
    "                (merged_data['Sector'].str.contains('Fishery', regex=True))          |\n",
    "                (merged_data['Sector'].str.contains('Defense & Space', regex=True))  |\n",
    "                (merged_data['Sector'].str.contains('arming', regex=True)) , 'Sector'] = 'Agriculture'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('—', regex=True)) , 'Sector'] = 'Undisclosed'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('Infra', regex=True))                    | \n",
    "                (merged_data['Sector'].str.contains('Renewables & Environment', regex=True)) |\n",
    "                (merged_data['Sector'].str.contains('Construction', regex=True)), 'Sector'] = 'Construction'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('B2B', regex=True))                   | \n",
    "                (merged_data['Sector'].str.contains('Automo', regex=True))                | \n",
    "                (merged_data['Sector'].str.contains('Sport', regex=True))                 |\n",
    "                (merged_data['Sector'].str.contains('Cosmetic', regex=True))              |\n",
    "                (merged_data['Sector'].str.contains('mmerce', regex=True))                | \n",
    "                (merged_data['Sector'].str.contains('Biomaterial', regex=True))           |\n",
    "                (merged_data['Sector'].str.contains('olar', regex=True))                  |\n",
    "                (merged_data['Sector'].str.contains('ailor', regex=True))                 |\n",
    "                (merged_data['Sector'].str.contains('Fashion', regex=True))               |\n",
    "                (merged_data['Sector'].str.contains('Trade', regex=True))                 | \n",
    "                (merged_data['Sector'].str.contains('Repair', regex=True))                |\n",
    "                (merged_data['Sector'].str.contains('onsumer', regex=True))               | \n",
    "                (merged_data['Sector'].str.contains('E store', regex=True))               |\n",
    "                (merged_data['Sector'].str.contains('E-marketplace', regex=True))         |\n",
    "                (merged_data['Sector'].str.contains('Tobacco', regex=True))               |\n",
    "                (merged_data['Sector'].str.contains('Furniture', regex=True))             |\n",
    "                (merged_data['Sector'].str.contains('Spiritual', regex=True))             |\n",
    "                (merged_data['Sector'].str.contains('Wine & Spirits', regex=True))        |\n",
    "                (merged_data['Sector'].str.contains('Estore', regex=True))                |\n",
    "                (merged_data['Sector'].str.contains('E-tail', regex=True))                |\n",
    "                (merged_data['Sector'].str.contains('Personal care startup', regex=True)) | \n",
    "                (merged_data['Sector'].str.contains('FMCG', regex=True))                  | \n",
    "                (merged_data['Sector'].str.contains('Linguistic Spiritual', regex=True))  |\n",
    "                (merged_data['Sector'].str.contains('E-marketplace', regex=True))         |\n",
    "                (merged_data['Sector'].str.contains('Eyeglasses', regex=True))            | \n",
    "                (merged_data['Sector'].str.contains('Jewellery', regex=True))             | \n",
    "                (merged_data['Sector'].str.contains('Reatil startup', regex=True))        | \n",
    "                (merged_data['Sector'].str.contains('Luxury car startup', regex=True))    | \n",
    "                (merged_data['Sector'].str.contains('Skincare startup', regex=True))      | \n",
    "                (merged_data['Sector'].str.contains('Dairy startup', regex=True))         | \n",
    "                (merged_data['Sector'].str.contains('Electronics', regex=True))           |\n",
    "                (merged_data['Sector'].str.contains('Wholesale', regex=True))             |\n",
    "                (merged_data['Sector'].str.contains('Pet care', regex=True))              | \n",
    "                (merged_data['Sector'].str.contains('Community', regex=True))             | \n",
    "                (merged_data['Sector'].str.contains('Cultural', regex=True))              | \n",
    "                (merged_data['Sector'].str.contains('Cannabis startup', regex=True))      | \n",
    "                (merged_data['Sector'].str.contains('D2C', regex=True))                   | \n",
    "                (merged_data['Sector'].str.contains('Arts & Crafts', regex=True))         | \n",
    "                (merged_data['Sector'].str.contains('Oil & Energy', regex=True))          |\n",
    "                (merged_data['Sector'].str.contains('arketplace', regex=True)), 'Sector'] = 'Commerce'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('Finance', regex=True))             |\n",
    "                   (merged_data['Sector'].str.contains('Financial', regex=True))        |\n",
    "                   (merged_data['Sector'].str.contains('Fintech', regex=True))          |\n",
    "                   (merged_data['Sector'].str.contains('FinTech', regex=True))          |\n",
    "                   (merged_data['Sector'].str.contains('anking', regex=True))           |\n",
    "                   (merged_data['Sector'].str.contains('API platform', regex=True))     |\n",
    "                   (merged_data['Sector'].str.contains('nsur', regex=True))             |\n",
    "                   (merged_data['Sector'].str.contains('Taxation', regex=True))         |\n",
    "                   (merged_data['Sector'].str.contains('Credit', regex=True))           |\n",
    "                   (merged_data['Sector'].str.contains('Digital mortgage', regex=True)) |\n",
    "                   (merged_data['Sector'].str.contains('Venture', regex=True))          |\n",
    "                   (merged_data['Sector'].str.contains('Trading', regex=True))          |\n",
    "                   (merged_data['Sector'].str.contains('auditing', regex=True))         | \n",
    "                   (merged_data['Sector'].str.contains('Mobile Payments', regex=True))  | \n",
    "                   (merged_data['Sector'].str.contains('Mutual', regex=True))           | \n",
    "                   (merged_data['Sector'].str.contains('Investment', regex=True)) , 'Sector' ] = 'Financial'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('Hauz Khas', regex=True)), 'HeadQuarter'] = 'Manchester'\n",
    "\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('IoT', regex=True))                     | \n",
    "                (merged_data['Sector'].str.contains('nternet', regex=True))                 |\n",
    "                (merged_data['Sector'].str.contains('Gam', regex=True))                     |\n",
    "                (merged_data['Sector'].str.contains('nforma', regex=True))                  | \n",
    "                (merged_data['Sector'].str.contains('echnology', regex=True))               |\n",
    "                (merged_data['Sector'].str.contains('elecom', regex=True))                  |\n",
    "                (merged_data['Sector'].str.contains('ngineering', regex=True))              | \n",
    "                (merged_data['Sector'].str.contains('Blockchain', regex=True))              |\n",
    "                (merged_data['Sector'].str.contains('Crypto', regex=True))                  |\n",
    "                (merged_data['Sector'].str.contains('Cyber', regex=True))                   | \n",
    "                (merged_data['Sector'].str.contains('IT', regex=True))                      |\n",
    "                (merged_data['Sector'].str.contains('Tech', regex=True))                    |\n",
    "                (merged_data['Sector'].str.contains('oftware', regex=True))                 | \n",
    "                (merged_data['Sector'].str.contains('aaS', regex=True))                     |\n",
    "                (merged_data['Sector'].str.contains('AI', regex=True))                      |\n",
    "                (merged_data['Sector'].str.contains('Artificial Intelligence', regex=True)) |\n",
    "                (merged_data['Sector'].str.contains('Big Data', regex=True))                | \n",
    "                (merged_data['Sector'].str.contains('Cloud', regex=True))                   |\n",
    "                (merged_data['Sector'].str.contains('Computing', regex=True))               |\n",
    "                (merged_data['Sector'].str.contains('App', regex=True))                     |\n",
    "                (merged_data['Sector'].str.contains('Automa', regex=True))                  |\n",
    "                (merged_data['Sector'].str.contains('Web', regex=True))                     |\n",
    "                (merged_data['Sector'].str.contains('AR/VR', regex=True))                   |\n",
    "                (merged_data['Sector'].str.contains('Data Intelligence', regex=True))       |\n",
    "                (merged_data['Sector'].str.contains('Data Analytics', regex=True))          |\n",
    "                (merged_data['Sector'].str.contains('MLOps platform', regex=True))          |\n",
    "                (merged_data['Sector'].str.contains('tech', regex=True))                    |\n",
    "                (merged_data['Sector'].str.contains('Cleantech', regex=True))               | \n",
    "                (merged_data['Sector'].str.contains('EV startup', regex=True))              | \n",
    "                (merged_data['Sector'].str.contains('AR', regex=True))                      | \n",
    "                (merged_data['Sector'].str.contains('Photonics', regex=True))               | \n",
    "                (merged_data['Sector'].str.contains('Spacetech', regex=True))               | \n",
    "                (merged_data['Sector'].str.contains('Mobile', regex=True))                  |\n",
    "                (merged_data['Sector'].str.contains('Renewable', regex=True))               | \n",
    "                (merged_data['Sector'].str.contains('Aero', regex=True))                    | \n",
    "                (merged_data['Sector'].str.contains('Drone', regex=True))                   | \n",
    "                (merged_data['Sector'].str.contains('Search Engine', regex=True))           | \n",
    "                (merged_data['Sector'].str.contains('Electric Vehicle', regex=True))        | \n",
    "                (merged_data['Sector'].str.contains('Augmented reality', regex=True))       |\n",
    "                (merged_data['Sector'].str.contains('Product studio', regex=True))          | \n",
    "                (merged_data['Sector'].str.contains('Network', regex=True))                 | \n",
    "                (merged_data['Sector'].str.contains('Online storytelling', regex=True))     |\n",
    "                (merged_data['Sector'].str.contains('Matrimony', regex=True))               |\n",
    "                (merged_data['Sector'].str.contains('Manchester', regex=True))              | \n",
    "                (merged_data['Sector'].str.contains('sports', regex=True))                  |\n",
    "                (merged_data['Sector'].str.contains('NFT', regex=True))                     |\n",
    "                (merged_data['Sector'].str.contains('Robotics', regex=True)), 'Sector'] = 'I.T & Engineering'\n",
    "\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('onglomerate ', regex=True)) , 'Sector' ] = 'Conglomerate'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('anufactu', regex=True)) |\n",
    "                (merged_data['Sector'].str.contains('Clothing', regex=True)) |\n",
    "                (merged_data['Sector'].str.contains('Textiles', regex=True)) |\n",
    "                (merged_data['Sector'].str.contains('ootwear', regex=True))  |\n",
    "                (merged_data['Sector'].str.contains('Battery design', regex=True)), 'Sector' ] = 'Manufacturing'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('Real Estate', regex=True)) |\n",
    "                (merged_data['Sector'].str.contains('Real estate', regex=True)) |\n",
    "                (merged_data['Sector'].str.contains('Warehouse', regex=True))   | \n",
    "                (merged_data['Sector'].str.contains('Proptech', regex=True))    | \n",
    "                (merged_data['Sector'].str.contains('Events', regex=True))      |\n",
    "                (merged_data['Sector'].str.contains('Accomodation', regex=True)), 'Sector' ] = 'Real Estate'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('edia', regex=True))                    |\n",
    "                   (merged_data['Sector'].str.contains('ublishing', regex=True))            |\n",
    "                   (merged_data['Sector'].str.contains('reaming', regex=True))              |\n",
    "                   (merged_data['Sector'].str.contains('OTT', regex=True))                  |\n",
    "                   (merged_data['Sector'].str.contains('Ad-tech', regex=True))              |\n",
    "                   (merged_data['Sector'].str.contains('ideo', regex=True))                 |\n",
    "                   (merged_data['Sector'].str.contains('dvert', regex=True))                |\n",
    "                   (merged_data['Sector'].str.contains('ntertain', regex=True))             |\n",
    "                   (merged_data['Sector'].str.contains('FM', regex=True))                   | \n",
    "                   (merged_data['Sector'].str.contains('Content Marktplace', regex=True))   | \n",
    "                   (merged_data['Sector'].str.contains('Music', regex=True))                | \n",
    "                   (merged_data['Sector'].str.contains('Blogging', regex=True))             | \n",
    "                   (merged_data['Sector'].str.contains('Content creation', regex=True))     |\n",
    "                   (merged_data['Sector'].str.contains('Podcast', regex=True))              |\n",
    "                   (merged_data['Sector'].str.contains('Celebrity Engagement', regex=True)) |\n",
    "                   (merged_data['Sector'].str.contains('Social audio', regex=True))         |\n",
    "                   (merged_data['Sector'].str.contains('Social network', regex=True))       | \n",
    "                   (merged_data['Sector'].str.contains('Digital platform', regex=True))     | \n",
    "                   (merged_data['Sector'].str.contains('Social platform', regex=True))      | \n",
    "                   (merged_data['Sector'].str.contains('ublication', regex=True)) , 'Sector' ] = 'Media'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('Hosp', regex=True))              |\n",
    "                   (merged_data['Sector'].str.contains('Tourism', regex=True))        |\n",
    "                   (merged_data['Sector'].str.contains('Travel', regex=True))         |\n",
    "                   (merged_data['Sector'].str.contains('Fantasy sports', regex=True)) |\n",
    "                   (merged_data['Sector'].str.contains('Housing', regex=True))        |\n",
    "                   (merged_data['Sector'].str.contains('Co-living', regex=True))      |\n",
    "                   (merged_data['Sector'].str.contains('Hotel', regex=True)) , 'Sector' ] = 'Hospitality'\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('Ed', regex=True))                  |\n",
    "                (merged_data['Sector'].str.contains('earning', regex=True))             |\n",
    "                (merged_data['Sector'].str.contains('Social Network', regex=True))      |\n",
    "                (merged_data['Sector'].str.contains('Networking platform', regex=True)) | \n",
    "                (merged_data['Sector'].str.contains('Preschool Daycare', regex=True))   |\n",
    "                (merged_data['Sector'].str.contains('Knowledge', regex=True)) , 'Sector'] = 'Education'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('Hauz Khas', regex=True)), 'HeadQuarter'] = 'Hauz Khas'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('Food', regex=True))                  |\n",
    "                   (merged_data['Sector'].str.contains('FoodTech', regex=True))           |\n",
    "                   (merged_data['Sector'].str.contains('Catering', regex=True))           |\n",
    "                   (merged_data['Sector'].str.contains('Beer', regex=True))               | \n",
    "                   (merged_data['Sector'].str.contains('QSR startup', regex=True))        | \n",
    "                   (merged_data['Sector'].str.contains('Milk startup', regex=True))       |\n",
    "                   (merged_data['Sector'].str.contains('Dairy', regex=True))              | \n",
    "                   (merged_data['Sector'].str.contains('Water purification', regex=True)) | \n",
    "                   (merged_data['Sector'].str.contains('Hauz Khas', regex=True))          |\n",
    "                   (merged_data['Sector'].str.contains('Beverage', regex=True)) , 'Sector' ] = 'Food/Culnary'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('elivery', regex=True))                |\n",
    "                   (merged_data['Sector'].str.contains('ransport', regex=True))            |\n",
    "                   (merged_data['Sector'].str.contains('ogistics', regex=True))            |\n",
    "                   (merged_data['Sector'].str.contains('Logitech', regex=True))            |\n",
    "                   (merged_data['Sector'].str.contains('space', regex=True))               |\n",
    "                   (merged_data['Sector'].str.contains('Electricity', regex=True))         |\n",
    "                   (merged_data['Sector'].str.contains('Maritime', regex=True))            |\n",
    "                   (merged_data['Sector'].str.contains('obility', regex=True))             |\n",
    "                   (merged_data['Sector'].str.contains('EV', regex=True))                  |\n",
    "                   (merged_data['Sector'].str.contains('Merchandise', regex=True))         |\n",
    "                   (merged_data['Sector'].str.contains('WL & RAC protection', regex=True)) |\n",
    "                   (merged_data['Sector'].str.contains('viation', regex=True))             |\n",
    "                   (merged_data['Sector'].str.contains('uppl', regex=True)) , 'Sector' ] = 'Logistics & Supply Chain'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('ellness', regex=True))                         |\n",
    "                   (merged_data['Sector'].str.contains('Hea', regex=True))                          |\n",
    "                   (merged_data['Sector'].str.contains('itness', regex=True))                       |\n",
    "                   (merged_data['Sector'].str.contains('yewear', regex=True))                       |\n",
    "                   (merged_data['Sector'].str.contains('Nutrition', regex=True))                    |\n",
    "                   (merged_data['Sector'].str.contains('Biotech', regex=True))                      |\n",
    "                   (merged_data['Sector'].str.contains('Eye Wear', regex=True))                     |\n",
    "                   (merged_data['Sector'].str.contains('Life sciences', regex=True))                |\n",
    "                   (merged_data['Sector'].str.contains('Pharma', regex=True))                       |\n",
    "                   (merged_data['Sector'].str.contains('pharma', regex=True))                       |\n",
    "                   (merged_data['Sector'].str.contains('Med', regex=True))                          |\n",
    "                   (merged_data['Sector'].str.contains('laboratory', regex=True))                   | \n",
    "                   (merged_data['Sector'].str.contains('Deeptech', regex=True))                     |\n",
    "                   (merged_data['Sector'].str.contains('Ayurveda tech', regex=True))                | \n",
    "                   (merged_data['Sector'].str.contains('Fertility tech', regex=True))               | \n",
    "                   (merged_data['Sector'].str.contains('Medic', regex=True))                        | \n",
    "                   (merged_data['Sector'].str.contains('Veterinary', regex=True))                   | \n",
    "                   (merged_data['Sector'].str.contains('Hygiene', regex=True))                      |\n",
    "                   (merged_data['Sector'].str.contains('Helathcare', regex=True))                   |\n",
    "                   (merged_data['Sector'].str.contains('Lifestyle', regex=True))                    |\n",
    "                   (merged_data['Sector'].str.contains('Personal Care', regex=True))                |\n",
    "                   (merged_data['Sector'].str.contains('Pollution control equiptment', regex=True)) |\n",
    "                   (merged_data['Sector'].str.contains('hea', regex=True)) , 'Sector' ] = 'Health'\n",
    "\n",
    "merged_data.loc[(merged_data['Sector'].str.contains('HR', regex=True))                              |\n",
    "                   (merged_data['Sector'].str.contains('Legal', regex=True))                        |\n",
    "                   (merged_data['Sector'].str.contains('Consulting', regex=True))                   |\n",
    "                   (merged_data['Sector'].str.contains('arket', regex=True))                        |\n",
    "                   (merged_data['Sector'].str.contains('Promotion', regex=True))                    |\n",
    "                   (merged_data['Sector'].str.contains('Rent', regex=True))                         |\n",
    "                   (merged_data['Sector'].str.contains('Retail', regex=True))                       |\n",
    "                   (merged_data['Sector'].str.contains('nterior', regex=True))                      |\n",
    "                   (merged_data['Sector'].str.contains('Design', regex=True))                       |\n",
    "                   (merged_data['Sector'].str.contains('Training', regex=True))                     |\n",
    "                   (merged_data['Sector'].str.contains('Transla', regex=True))                      |\n",
    "                   (merged_data['Sector'].str.contains('ervice', regex=True))                       |\n",
    "                   (merged_data['Sector'].str.contains('olution', regex=True))                      |\n",
    "                   (merged_data['Sector'].str.contains('ecruit', regex=True))                       |\n",
    "                   (merged_data['Sector'].str.contains('anagement', regex=True))                    |\n",
    "                   (merged_data['Sector'].str.contains('Sales', regex=True))                        |\n",
    "                   (merged_data['Sector'].str.contains('CRM', regex=True))                          |\n",
    "                   (merged_data['Sector'].str.contains('Beauty', regex=True))                       |\n",
    "                   (merged_data['Sector'].str.contains('Location Analytics', regex=True))           |\n",
    "                   (merged_data['Sector'].str.contains('Oil and Energy', regex=True))               |\n",
    "                   (merged_data['Sector'].str.contains('Skill development', regex=True))            |\n",
    "                   (merged_data['Sector'].str.contains('Deisgning', regex=True))                    |\n",
    "                   (merged_data['Sector'].str.contains('E-connect', regex=True))                    |\n",
    "                   (merged_data['Sector'].str.contains('Resources', regex=True))                    |\n",
    "                   (merged_data['Sector'].str.contains('Entreprenurship', regex=True))              | \n",
    "                   (merged_data['Sector'].str.contains('Co-working', regex=True))                   | \n",
    "                   (merged_data['Sector'].str.contains('oworking', regex=True))                     | \n",
    "                   (merged_data['Sector'].str.contains('Autonomous Vehicles', regex=True))          | \n",
    "                   (merged_data['Sector'].str.contains('Work fulfillment', regex=True))             | \n",
    "                   (merged_data['Sector'].str.contains('Job', regex=True))                          | \n",
    "                   (merged_data['Sector'].str.contains('Crowdsourcing', regex=True))                |\n",
    "                   (merged_data['Sector'].str.contains('Safety tech', regex=True))                  |  \n",
    "                   (merged_data['Sector'].str.contains('Escrow', regex=True))                       | \n",
    "                   (merged_data['Sector'].str.contains('Funding Platform, Incubators', regex=True)) | \n",
    "                   (merged_data['Sector'].str.contains('Wedding', regex=True))                      | \n",
    "                   (merged_data['Sector'].str.contains('Advisory', regex=True))                     |\n",
    "                   (merged_data['Sector'].str.contains('Home Decor', regex=True))                   |\n",
    "                   (merged_data['Sector'].str.contains('Vehicle repair startup', regex=True))       |\n",
    "                   (merged_data['Sector'].str.contains('Resources', regex=True))                    | \n",
    "                   (merged_data['Sector'].str.contains('Social community', regex=True))             | \n",
    "                   (merged_data['Sector'].str.contains('Analytics', regex=True))                    |\n",
    "                   (merged_data['Sector'].str.contains('Battery', regex=True))                      |\n",
    "                   (merged_data['Sector'].str.contains('Dating', regex=True)), 'Sector' ] = 'Services'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "merged_data['Sector'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
